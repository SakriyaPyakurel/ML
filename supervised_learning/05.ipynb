{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324592ba",
   "metadata": {},
   "source": [
    "## Metrics - Overview\n",
    "\n",
    "Useful to evaluate how precise and accurate a ml model is.<br> \n",
    "Two types of evaluation metrics are there:<br>\n",
    "\n",
    "i. Regression metrics<br>\n",
    "ii. Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993fdbc",
   "metadata": {},
   "source": [
    "# Regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c9da9",
   "metadata": {},
   "source": [
    "a. Mean squared error(mse) : Average squared difference between predicted and actual values.\n",
    "\n",
    "MSE = (1/n) * Σ (predicted value - actual value)² <br>\n",
    "\n",
    "n is the number of data points.<br>\n",
    "Σ represents the summation of all squared differences.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870fe3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "from sklearn.linear_model import LinearRegression \n",
    "import seaborn as sb \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error\n",
    "# loading the dataset\n",
    "df = sb.load_dataset('tips') \n",
    "# selecting features and target variable \n",
    "X = df[['total_bill', 'size']]\n",
    "y = df['tip'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5688173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.89943693, 1.89520002, 3.85933994, 3.9811207 , 2.27962736])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the dataset into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "# creating a linear regression model\n",
    "model = LinearRegression() \n",
    "# fitting the model to the training data\n",
    "model.fit(X_train, y_train) \n",
    "# making predictions on the test data \n",
    "y_pred = model.predict(X_test) \n",
    "# displaying the made predictions \n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09cab82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6485996190543517"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the mean squared error \n",
    "mse = mean_squared_error(y_test, y_pred) \n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19b084",
   "metadata": {},
   "source": [
    "<b>Interpretation:</b> The average error or deviation of predicted value from the original values is about 0.65 according to mse(mean squared error) indicating good model performance but still room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5afdc2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8053568271607013)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating root mean squared error through custom function \n",
    "import numpy as np\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred)) \n",
    "rmse = root_mean_squared_error(y_test, y_pred) \n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86bf90b",
   "metadata": {},
   "source": [
    "b. Mean absolute error(mae) : calculates the average of the absolute differences between predicted and actual values along with providing a straightforward way to quantify the errors.<br>\n",
    "\n",
    "MAE = (1/n) * Σ |predicted value - actual value|<br>\n",
    "\n",
    "n is the number of data points.<br>\n",
    "Σ represents the summation of all squared differences.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a847f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6639235737596481"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the mean absolute error \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "mae = mean_absolute_error(y_test, y_pred) \n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab61fc4",
   "metadata": {},
   "source": [
    "<b>Interpretation:</b> The average error or deviation of predicted value from the original values is about 0.65 according to mae(mean absolute error) indicating good model performance but still room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954392dc",
   "metadata": {},
   "source": [
    "c. Mean absolute percentage error(mape):  expresses the average percentage difference between predicted and actual values providing a relative measure of error which is very easy to understand.<br>\n",
    "\n",
    "MAPE = (1/n) * Σ |(actual value - predicted value) / actual value| * 100<br>\n",
    "\n",
    "n is the number of data points.<br>\n",
    "Σ represents the summation of all squared differences.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c658b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(27.978917485910255)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the mean absolute percentage error from custom made function \n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred) \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred) \n",
    "mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b760c5",
   "metadata": {},
   "source": [
    "<b>Interpretation:</b> According to mean absolute percentage error(mape) about 28% of the predictions done by the model is errorful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed311036",
   "metadata": {},
   "source": [
    "d. Mean squared logarithmic error(msle) : Improved version of mean sqaured error(mse) but with logarithmic functions when the data has wide(varied) range of values.<br>\n",
    "\n",
    "MSLE = (1/n) * Σ [log(predicted_value - actual value)] <br>\n",
    "\n",
    "n is the number of data points.<br>\n",
    "Σ represents the summation of all squared differences.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869171a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.04365490227864684)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the mean sqaured logarithmic error from custom made function \n",
    "def mean_squared_logarithmic_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred) \n",
    "    return np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true)))\n",
    "msle = mean_squared_logarithmic_error(y_test, y_pred) \n",
    "msle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73129c19",
   "metadata": {},
   "source": [
    "<b>Interpretation:</b> mean sqaured logarithmic error(msle) is 0.04 determining the ability of the model to handle most varied values in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135722b",
   "metadata": {},
   "source": [
    "e. r2_score : Determines the proportion of the variance in the dependent variable that is predictable from the independent variables. A good r2_score is often considered to be in range of 70 to 99.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8693cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4811084097989491"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating r2_score \n",
    "from sklearn.metrics import r2_score \n",
    "r2 = r2_score(y_test, y_pred) \n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b0f30",
   "metadata": {},
   "source": [
    "<b>Interpretation:</b> The r2_score is only 0.48 which is non satisfactory indicating the model's inability to predict the dependent variable('tip') based on two independent variables('total_bill' & 'size'). Only 48% of the total variance in the dependent variable is predicted through the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e3825",
   "metadata": {},
   "source": [
    "# Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c574fe8",
   "metadata": {},
   "source": [
    "a. accuracy_score : evaluates the exact state of matching between the predicted and actual labels.<br>\n",
    "\n",
    "Binary classification : accuracy_score = (True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False Negatives) <br>\n",
    "\n",
    "True Positives (TP): Correctly predicted positive.<br>\n",
    "True Negatives (TN): Correctly predicted negative.<br>\n",
    "False Positives (FP): Incorrectly predicted positive (Type I error).<br>\n",
    "False Negatives (FN): Incorrectly predicted negative (Type II error).<br>\n",
    "\n",
    "Multilabel classification : accuracy_score = (Number of Correct Predictions) / (Total Number of Predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb99ae59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset \n",
    "df = sb.load_dataset('titanic') \n",
    "# selecting features and target variable for binary classification\n",
    "df = df.dropna(subset=['age', 'fare', 'survived'])  # dropping rows with NaN values in selected columns\n",
    "X = df[['age', 'fare']] \n",
    "y = df['survived'] \n",
    "# splitting the dataset into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# creating a logistic  regression model\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model = LogisticRegression() \n",
    "# fitting the model to the training data \n",
    "model.fit(X_train, y_train)\n",
    "# making predictions on the test data \n",
    "y_pred = model.predict(X_test) \n",
    "# displaying the made predictions\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75eb17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6293706293706294"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score, f1_score  # precision_score will be implemented with custom function\n",
    "# calculating accuracy \n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc3857",
   "metadata": {},
   "source": [
    "<b>Interpretation:</b> According to the accuracy_score of about 0.63 (~0.62) , about 63 times out of 100 the label is predicted correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c8bae",
   "metadata": {},
   "source": [
    "b. precision_score : measures the accuracy of positive predictions.\n",
    "\n",
    "precision_score = True Positives / (True Positives + False Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775aec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5483870967741935)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing precision score with custom function \n",
    "def precision_score(y_true, y_pred):\n",
    "    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    false_positive = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    return true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "precision = precision_score(y_test, y_pred)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a212b3",
   "metadata": {},
   "source": [
    "<b>Interpretation</b> According to the precision_score of 54.83, about 55 percent of the test data is predicted successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d9a5c",
   "metadata": {},
   "source": [
    "c. recall_score : a metric to evaluate how well a model identifies positive instances.<br>\n",
    "\n",
    "recall_score = TP / (TP + FN)<br>\n",
    "\n",
    "TP : True Positives<br>\n",
    "FN : False Negatives<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0badb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30357142857142855"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating recall score \n",
    "recall = recall_score(y_test, y_pred)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466533c",
   "metadata": {},
   "source": [
    "<b>Interpretation:</b> recall_score of 0.30 shows that only 30% of the positives are identified by the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
